# generated by datamodel-codegen:
#   filename:  openapi.yaml
#   timestamp: 2021-12-31T02:54:46+00:00

from __future__ import annotations

from datetime import datetime
from enum import Enum
from typing import Annotated, Any, List, Optional

from pydantic import BaseModel, Extra, Field


class InvalidParameterException(BaseModel):
    __root__: Any


class InvalidS3ObjectException(InvalidParameterException):
    pass


class ImageTooLargeException(InvalidParameterException):
    pass


class AccessDeniedException(InvalidParameterException):
    pass


class InternalServerError(InvalidParameterException):
    pass


class ThrottlingException(InvalidParameterException):
    pass


class ProvisionedThroughputExceededException(InvalidParameterException):
    pass


class InvalidImageFormatException(InvalidParameterException):
    pass


class ResourceAlreadyExistsException(InvalidParameterException):
    pass


class ServiceQuotaExceededException(InvalidParameterException):
    pass


class ResourceInUseException(InvalidParameterException):
    pass


class LimitExceededException(InvalidParameterException):
    pass


class ResourceNotFoundException(InvalidParameterException):
    pass


class DeleteStreamProcessorResponse(BaseModel):
    pass


class InvalidPaginationTokenException(InvalidParameterException):
    pass


class ResourceNotReadyException(InvalidParameterException):
    pass


class HumanLoopQuotaExceededException(InvalidParameterException):
    pass


class IdempotentParameterMismatchException(InvalidParameterException):
    pass


class VideoTooLargeException(InvalidParameterException):
    pass


class StartStreamProcessorResponse(DeleteStreamProcessorResponse):
    pass


class StopStreamProcessorResponse(DeleteStreamProcessorResponse):
    pass


class TagResourceResponse(DeleteStreamProcessorResponse):
    pass


class UntagResourceResponse(DeleteStreamProcessorResponse):
    pass


class UInteger(BaseModel):
    __root__: Annotated[int, Field(ge=0.0)]


class AgeRange(BaseModel):
    """
    <p>Structure containing the estimated age range, in years, for a face.</p> <p>Amazon Rekognition estimates an age range for faces detected in the input image. Estimated age ranges can overlap. A face of a 5-year-old might have an estimated range of 4-6, while the face of a 6-year-old might have an estimated range of 4-8.</p>
    """

    Low: Optional[UInteger] = None
    High: Optional[UInteger] = None


class Attribute(Enum):
    DEFAULT = 'DEFAULT'
    ALL = 'ALL'


class Attributes(BaseModel):
    __root__: List[Attribute]


class String(BaseModel):
    __root__: str


class ULong(UInteger):
    pass


class AudioMetadata1(BaseModel):
    """
    Metadata information about an audio stream. An array of <code>AudioMetadata</code> objects for the audio streams found in a stored video is returned by <a>GetSegmentDetection</a>.
    """

    Codec: Optional[String] = None
    DurationMillis: Optional[ULong] = None
    SampleRate: Optional[ULong] = None
    NumberOfChannels: Optional[ULong] = None


class AudioMetadataList(BaseModel):
    __root__: List[AudioMetadata1]


class Boolean(BaseModel):
    __root__: bool


class Percent(BaseModel):
    __root__: Annotated[float, Field(ge=0.0, le=100.0)]


class Beard(BaseModel):
    """
    Indicates whether or not the face has a beard, and the confidence level in the determination.
    """

    Value: Optional[Boolean] = None
    Confidence: Optional[Percent] = None


class MaxPixelThreshold(BaseModel):
    __root__: Annotated[float, Field(ge=0.0, le=1.0)]


class MinCoveragePercentage(Percent):
    pass


class BlackFrame(BaseModel):
    """
    A filter that allows you to control the black frame detection by specifying the black levels and pixel coverage of black pixels in a frame. As videos can come from multiple sources, formats, and time periods, they may contain different standards and varying noise levels for black frames that need to be accounted for. For more information, see <a>StartSegmentDetection</a>.
    """

    MaxPixelThreshold: Optional[MaxPixelThreshold] = None
    MinCoveragePercentage: Optional[MinCoveragePercentage] = None


class BodyPart(Enum):
    FACE = 'FACE'
    HEAD = 'HEAD'
    LEFT_HAND = 'LEFT_HAND'
    RIGHT_HAND = 'RIGHT_HAND'


class Float(BaseModel):
    __root__: float


class BoundingBox(BaseModel):
    """
    <p>Identifies the bounding box around the label, face, text or personal protective equipment. The <code>left</code> (x-coordinate) and <code>top</code> (y-coordinate) are coordinates representing the top and left sides of the bounding box. Note that the upper-left corner of the image is the origin (0,0). </p> <p>The <code>top</code> and <code>left</code> values returned are ratios of the overall image size. For example, if the input image is 700x200 pixels, and the top-left coordinate of the bounding box is 350x50 pixels, the API returns a <code>left</code> value of 0.5 (350/700) and a <code>top</code> value of 0.25 (50/200).</p> <p>The <code>width</code> and <code>height</code> values represent the dimensions of the bounding box as a ratio of the overall image dimension. For example, if the input image is 700x200 pixels, and the bounding box width is 70 pixels, the width returned is 0.1. </p> <note> <p> The bounding box coordinates can have negative values. For example, if Amazon Rekognition is able to detect a face that is at the image edge and is only partially visible, the service can return coordinates that are outside the image bounds and, depending on the image edge, you might get negative values or values greater than 1 for the <code>left</code> or <code>top</code> values. </p> </note>
    """

    Width: Optional[Float] = None
    Height: Optional[Float] = None
    Left: Optional[Float] = None
    Top: Optional[Float] = None


class BoundingBoxHeight(MaxPixelThreshold):
    pass


class BoundingBoxWidth(MaxPixelThreshold):
    pass


class RekognitionUniqueId(BaseModel):
    __root__: Annotated[str, Field(regex='[0-9A-Za-z]*')]


class Timestamp(BaseModel):
    __root__: int


class CelebrityRecognitionSortBy(Enum):
    ID = 'ID'
    TIMESTAMP = 'TIMESTAMP'


class ClientRequestToken(BaseModel):
    __root__: Annotated[
        str, Field(max_length=64, min_length=1, regex='^[a-zA-Z0-9-_]+$')
    ]


class CollectionId(BaseModel):
    __root__: Annotated[
        str, Field(max_length=255, min_length=1, regex='[a-zA-Z0-9_.\\-]+')
    ]


class CollectionIdList(BaseModel):
    __root__: List[CollectionId]


class QualityFilter(Enum):
    NONE = 'NONE'
    AUTO = 'AUTO'
    LOW = 'LOW'
    MEDIUM = 'MEDIUM'
    HIGH = 'HIGH'


class ComparedSourceImageFace(BaseModel):
    """
    Type that describes the face Amazon Rekognition chose to compare with the faces in the target. This contains a bounding box for the selected face and confidence level that the bounding box contains a face. Note that Amazon Rekognition selects the largest face in the source image for this comparison.
    """

    BoundingBox: Optional[BoundingBox] = None
    Confidence: Optional[Percent] = None


class OrientationCorrection(Enum):
    ROTATE_0 = 'ROTATE_0'
    ROTATE_90 = 'ROTATE_90'
    ROTATE_180 = 'ROTATE_180'
    ROTATE_270 = 'ROTATE_270'


class ImageQuality(BaseModel):
    """
    Identifies face image brightness and sharpness.
    """

    Brightness: Optional[Float] = None
    Sharpness: Optional[Float] = None


class Smile(Beard):
    """
    Indicates whether or not the face is smiling, and the confidence level in the determination.
    """

    pass


class ContentClassifier(Enum):
    FreeOfPersonallyIdentifiableInformation = 'FreeOfPersonallyIdentifiableInformation'
    FreeOfAdultContent = 'FreeOfAdultContent'


class ContentClassifiers(BaseModel):
    __root__: Annotated[List[ContentClassifier], Field(max_items=256)]


class ModerationLabel(BaseModel):
    """
    Provides information about a single type of inappropriate, unwanted, or offensive content found in an image or video. Each type of moderated content has a label within a hierarchical taxonomy. For more information, see Content moderation in the Amazon Rekognition Developer Guide.
    """

    Confidence: Optional[Percent] = None
    Name: Optional[String] = None
    ParentName: Optional[String] = None


class ContentModerationDetection(BaseModel):
    """
    Information about an inappropriate, unwanted, or offensive content label detection in a stored video.
    """

    Timestamp: Optional[Timestamp] = None
    ModerationLabel: Optional[ModerationLabel] = None


class ContentModerationDetections(BaseModel):
    __root__: List[ContentModerationDetection]


class ContentModerationSortBy(Enum):
    NAME = 'NAME'
    TIMESTAMP = 'TIMESTAMP'


class CoversBodyPart(BaseModel):
    """
    Information about an item of Personal Protective Equipment covering a corresponding body part. For more information, see <a>DetectProtectiveEquipment</a>.
    """

    Confidence: Optional[Percent] = None
    Value: Optional[Boolean] = None


class TagMap(BaseModel):
    pass

    class Config:
        extra = Extra.allow


class ProjectName(CollectionId):
    pass


class ProjectArn(BaseModel):
    __root__: Annotated[
        str,
        Field(
            max_length=2048,
            min_length=20,
            regex='(^arn:[a-z\\d-]+:rekognition:[a-z\\d-]+:\\d{12}:project\\/[a-zA-Z0-9_.\\-]{1,255}\\/[0-9]+$)',
        ),
    ]


class VersionName(CollectionId):
    pass


class KmsKeyId(BaseModel):
    __root__: Annotated[
        str,
        Field(
            max_length=2048,
            min_length=1,
            regex='^[A-Za-z0-9][A-Za-z0-9:_/+=,@.-]{0,2048}$',
        ),
    ]


class ProjectVersionArn(BaseModel):
    __root__: Annotated[
        str,
        Field(
            max_length=2048,
            min_length=20,
            regex='(^arn:[a-z\\d-]+:rekognition:[a-z\\d-]+:\\d{12}:project\\/[a-zA-Z0-9_.\\-]{1,255}\\/version\\/[a-zA-Z0-9_.\\-]{1,255}\\/[0-9]+$)',
        ),
    ]


class StreamProcessorName(BaseModel):
    __root__: Annotated[
        str, Field(max_length=128, min_length=1, regex='[a-zA-Z0-9_.\\-]+')
    ]


class RoleArn(BaseModel):
    __root__: Annotated[
        str, Field(regex='arn:aws:iam::\\d{12}:role/?[a-zA-Z_0-9+=,.@\\-_/]+')
    ]


class StreamProcessorArn(BaseModel):
    __root__: Annotated[
        str,
        Field(
            regex='(^arn:[a-z\\d-]+:rekognition:[a-z\\d-]+:\\d{12}:streamprocessor\\/.+$)'
        ),
    ]


class DateTime(BaseModel):
    __root__: datetime


class Degree(BaseModel):
    __root__: Annotated[float, Field(ge=-180.0, le=180.0)]


class ProjectStatus(Enum):
    CREATING = 'CREATING'
    CREATED = 'CREATED'
    DELETING = 'DELETING'


class ProjectVersionStatus(Enum):
    TRAINING_IN_PROGRESS = 'TRAINING_IN_PROGRESS'
    TRAINING_COMPLETED = 'TRAINING_COMPLETED'
    TRAINING_FAILED = 'TRAINING_FAILED'
    STARTING = 'STARTING'
    RUNNING = 'RUNNING'
    FAILED = 'FAILED'
    STOPPING = 'STOPPING'
    STOPPED = 'STOPPED'
    DELETING = 'DELETING'


class VersionNames(BaseModel):
    __root__: Annotated[List[VersionName], Field(max_items=10, min_items=1)]


class ExtendedPaginationToken(BaseModel):
    __root__: Annotated[str, Field(max_length=1024)]


class ProjectVersionsPageSize(BaseModel):
    __root__: Annotated[int, Field(ge=1.0, le=100.0)]


class ProjectsPageSize(ProjectVersionsPageSize):
    pass


class StreamProcessorStatus(Enum):
    STOPPED = 'STOPPED'
    STARTING = 'STARTING'
    RUNNING = 'RUNNING'
    FAILED = 'FAILED'
    STOPPING = 'STOPPING'


class ModerationLabels(BaseModel):
    __root__: List[ModerationLabel]


class DetectionFilter(BaseModel):
    """
    A set of parameters that allow you to filter out certain results from your returned results.
    """

    MinConfidence: Optional[Percent] = None
    MinBoundingBoxHeight: Optional[BoundingBoxHeight] = None
    MinBoundingBoxWidth: Optional[BoundingBoxWidth] = None


class EmotionName(Enum):
    HAPPY = 'HAPPY'
    SAD = 'SAD'
    ANGRY = 'ANGRY'
    CONFUSED = 'CONFUSED'
    DISGUSTED = 'DISGUSTED'
    SURPRISED = 'SURPRISED'
    CALM = 'CALM'
    UNKNOWN = 'UNKNOWN'
    FEAR = 'FEAR'


class Emotion(BaseModel):
    """
    The emotions that appear to be expressed on the face, and the confidence level in the determination. The API is only making a determination of the physical appearance of a person's face. It is not a determination of the person’s internal emotional state and should not be used in such a way. For example, a person pretending to have a sad face might not be sad emotionally.
    """

    Type: Optional[EmotionName] = None
    Confidence: Optional[Percent] = None


class ProtectiveEquipmentType(Enum):
    FACE_COVER = 'FACE_COVER'
    HAND_COVER = 'HAND_COVER'
    HEAD_COVER = 'HEAD_COVER'


class EquipmentDetection(BaseModel):
    """
    Information about an item of Personal Protective Equipment (PPE) detected by <a>DetectProtectiveEquipment</a>. For more information, see <a>DetectProtectiveEquipment</a>.
    """

    BoundingBox: Optional[BoundingBox] = None
    Confidence: Optional[Percent] = None
    Type: Optional[ProtectiveEquipmentType] = None
    CoversBodyPart: Optional[CoversBodyPart] = None


class EquipmentDetections(BaseModel):
    __root__: List[EquipmentDetection]


class ExternalImageId(BaseModel):
    __root__: Annotated[
        str, Field(max_length=255, min_length=1, regex='[a-zA-Z0-9_.\\-:]+')
    ]


class EyeOpen(Beard):
    """
    Indicates whether or not the eyes on the face are open, and the confidence level in the determination.
    """

    pass


class Eyeglasses(Beard):
    """
    Indicates whether or not the face is wearing eye glasses, and the confidence level in the determination.
    """

    pass


class FaceId(BaseModel):
    __root__: Annotated[
        str, Field(regex='[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}')
    ]


class ImageId(FaceId):
    pass


class Face3(BaseModel):
    """
    Describes the face properties such as the bounding box, face ID, image ID of the input image, and external image ID that you assigned.
    """

    FaceId: Optional[FaceId] = None
    BoundingBox: Optional[BoundingBox] = None
    ImageId: Optional[ImageId] = None
    ExternalImageId: Optional[ExternalImageId] = None
    Confidence: Optional[Percent] = None


class Sunglasses(Beard):
    """
    Indicates whether or not the face is wearing sunglasses, and the confidence level in the determination.
    """

    pass


class Mustache(Beard):
    """
    Indicates whether or not the face has a mustache, and the confidence level in the determination.
    """

    pass


class MouthOpen(Beard):
    """
    Indicates whether or not the mouth on the face is open, and the confidence level in the determination.
    """

    pass


class FaceList(BaseModel):
    __root__: List[Face3]


class FaceMatch(BaseModel):
    """
    Provides face metadata. In addition, it also provides the confidence in the match of this face with the input face.
    """

    Similarity: Optional[Percent] = None
    Face: Optional[Face3] = None


class FaceMatchList(BaseModel):
    __root__: List[FaceMatch]


class FaceModelVersionList(BaseModel):
    __root__: List[String]


class FaceSearchSettings(BaseModel):
    """
    Input face recognition parameters for an Amazon Rekognition stream processor. <code>FaceRecognitionSettings</code> is a request parameter for <a>CreateStreamProcessor</a>.
    """

    CollectionId: Optional[CollectionId] = None
    FaceMatchThreshold: Optional[Percent] = None


class FaceSearchSortBy(Enum):
    INDEX = 'INDEX'
    TIMESTAMP = 'TIMESTAMP'


class FlowDefinitionArn(BaseModel):
    __root__: Annotated[str, Field(max_length=256)]


class GenderType(Enum):
    Male = 'Male'
    Female = 'Female'


class JobId(ClientRequestToken):
    pass


class MaxResults(BaseModel):
    __root__: Annotated[int, Field(ge=1.0)]


class PaginationToken(BaseModel):
    __root__: Annotated[str, Field(max_length=255)]


class VideoJobStatus(Enum):
    IN_PROGRESS = 'IN_PROGRESS'
    SUCCEEDED = 'SUCCEEDED'
    FAILED = 'FAILED'


class StatusMessage(String):
    pass


class HumanLoopActivationConditionsEvaluationResults(BaseModel):
    __root__: Annotated[str, Field(max_length=10240)]


class HumanLoopArn(FlowDefinitionArn):
    pass


class HumanLoopActivationReason(String):
    pass


class HumanLoopName(BaseModel):
    __root__: Annotated[
        str, Field(max_length=63, min_length=1, regex='^[a-z0-9](-*[a-z0-9])*')
    ]


class HumanLoopDataAttributes(BaseModel):
    """
    Allows you to set attributes of the image. Currently, you can declare an image as free of personally identifiable information.
    """

    ContentClassifiers: Optional[ContentClassifiers] = None


class ImageBlob(BaseModel):
    __root__: Annotated[str, Field(max_length=5242880, min_length=1)]


class MaxFacesToIndex(MaxResults):
    pass


class InferenceUnits(MaxResults):
    pass


class Instance(ComparedSourceImageFace):
    """
    An instance of a label returned by Amazon Rekognition Image (<a>DetectLabels</a>) or by Amazon Rekognition Video (<a>GetLabelDetection</a>).
    """

    pass


class Instances(BaseModel):
    __root__: List[Instance]


class JobTag(BaseModel):
    __root__: Annotated[
        str, Field(max_length=256, min_length=1, regex='[a-zA-Z0-9_.\\-:]+')
    ]


class KinesisDataArn(BaseModel):
    __root__: Annotated[
        str, Field(regex='(^arn:([a-z\\d-]+):kinesis:([a-z\\d-]+):\\d{12}:.+$)')
    ]


class KinesisDataStream(BaseModel):
    """
    The Kinesis data stream Amazon Rekognition to which the analysis results of a Amazon Rekognition stream processor are streamed. For more information, see CreateStreamProcessor in the Amazon Rekognition Developer Guide.
    """

    Arn: Optional[KinesisDataArn] = None


class KinesisVideoArn(BaseModel):
    __root__: Annotated[
        str, Field(regex='(^arn:([a-z\\d-]+):kinesisvideo:([a-z\\d-]+):\\d{12}:.+$)')
    ]


class KinesisVideoStream(BaseModel):
    """
    Kinesis video stream stream that provides the source streaming video for a Amazon Rekognition Video stream processor. For more information, see CreateStreamProcessor in the Amazon Rekognition Developer Guide.
    """

    Arn: Optional[KinesisVideoArn] = None


class LandmarkType(Enum):
    eyeLeft = 'eyeLeft'
    eyeRight = 'eyeRight'
    nose = 'nose'
    mouthLeft = 'mouthLeft'
    mouthRight = 'mouthRight'
    leftEyeBrowLeft = 'leftEyeBrowLeft'
    leftEyeBrowRight = 'leftEyeBrowRight'
    leftEyeBrowUp = 'leftEyeBrowUp'
    rightEyeBrowLeft = 'rightEyeBrowLeft'
    rightEyeBrowRight = 'rightEyeBrowRight'
    rightEyeBrowUp = 'rightEyeBrowUp'
    leftEyeLeft = 'leftEyeLeft'
    leftEyeRight = 'leftEyeRight'
    leftEyeUp = 'leftEyeUp'
    leftEyeDown = 'leftEyeDown'
    rightEyeLeft = 'rightEyeLeft'
    rightEyeRight = 'rightEyeRight'
    rightEyeUp = 'rightEyeUp'
    rightEyeDown = 'rightEyeDown'
    noseLeft = 'noseLeft'
    noseRight = 'noseRight'
    mouthUp = 'mouthUp'
    mouthDown = 'mouthDown'
    leftPupil = 'leftPupil'
    rightPupil = 'rightPupil'
    upperJawlineLeft = 'upperJawlineLeft'
    midJawlineLeft = 'midJawlineLeft'
    chinBottom = 'chinBottom'
    midJawlineRight = 'midJawlineRight'
    upperJawlineRight = 'upperJawlineRight'


class Landmark(BaseModel):
    """
    Indicates the location of the landmark on the face.
    """

    Type: Optional[LandmarkType] = None
    X: Optional[Float] = None
    Y: Optional[Float] = None


class PageSize(BaseModel):
    __root__: Annotated[int, Field(ge=0.0, le=4096.0)]


class ResourceArn(BaseModel):
    __root__: Annotated[str, Field(max_length=2048, min_length=20)]


class MaxFaces(BaseModel):
    __root__: Annotated[int, Field(ge=1.0, le=4096.0)]


class SNSTopicArn(BaseModel):
    __root__: Annotated[str, Field(regex='(^arn:aws:sns:.*:\\w{12}:.+$)')]


class NotificationChannel(BaseModel):
    """
    The Amazon Simple Notification Service topic to which Amazon Rekognition publishes the completion status of a video analysis operation. For more information, see <a>api-video</a>. Note that the Amazon SNS topic must have a topic name that begins with <i>AmazonRekognition</i> if you are using the AmazonRekognitionServiceRole permissions policy to access the topic. For more information, see <a href="https://docs.aws.amazon.com/rekognition/latest/dg/api-video-roles.html#api-video-roles-all-topics">Giving access to multiple Amazon SNS topics</a>.
    """

    SNSTopicArn: SNSTopicArn
    RoleArn: RoleArn


class S3Bucket(BaseModel):
    __root__: Annotated[
        str, Field(max_length=255, min_length=3, regex='[0-9A-Za-z\\.\\-_]*')
    ]


class S3KeyPrefix(ExtendedPaginationToken):
    pass


class Parent(BaseModel):
    """
    A parent label for a label. A label can have 0, 1, or more parents.
    """

    Name: Optional[String] = None


class PersonIndex(Timestamp):
    pass


class Point(BaseModel):
    """
    <p>The X and Y coordinates of a point on an image. The X and Y values returned are ratios of the overall image size. For example, if the input image is 700x200 and the operation returns X=0.5 and Y=0.25, then the point is at the (350,50) pixel coordinate on the image.</p> <p>An array of <code>Point</code> objects, <code>Polygon</code>, is returned by <a>DetectText</a> and by <a>DetectCustomLabels</a>. <code>Polygon</code> represents a fine-grained polygon around a detected item. For more information, see Geometry in the Amazon Rekognition Developer Guide. </p>
    """

    X: Optional[Float] = None
    Y: Optional[Float] = None


class ProjectDescription(BaseModel):
    """
    A description of a Amazon Rekognition Custom Labels project.
    """

    ProjectArn: Optional[ProjectArn] = None
    CreationTimestamp: Optional[DateTime] = None
    Status: Optional[ProjectStatus] = None


class ProtectiveEquipmentPersonIds(BaseModel):
    __root__: List[UInteger]


class ProtectiveEquipmentTypes(BaseModel):
    __root__: List[ProtectiveEquipmentType]


class Reason(Enum):
    EXCEEDS_MAX_FACES = 'EXCEEDS_MAX_FACES'
    EXTREME_POSE = 'EXTREME_POSE'
    LOW_BRIGHTNESS = 'LOW_BRIGHTNESS'
    LOW_SHARPNESS = 'LOW_SHARPNESS'
    LOW_CONFIDENCE = 'LOW_CONFIDENCE'
    SMALL_BOUNDING_BOX = 'SMALL_BOUNDING_BOX'
    LOW_FACE_QUALITY = 'LOW_FACE_QUALITY'


class Reasons(BaseModel):
    __root__: List[Reason]


class RegionOfInterest(BaseModel):
    """
    <p>Specifies a location within the frame that Rekognition checks for text. Uses a <code>BoundingBox</code> object to set a region of the screen.</p> <p>A word is included in the region if the word is more than half in that region. If there is more than one region, the word will be compared with all regions of the screen. Any word more than half in a region is kept in the results.</p>
    """

    BoundingBox: Optional[BoundingBox] = None


class S3ObjectName(BaseModel):
    __root__: Annotated[str, Field(max_length=1024, min_length=1)]


class S3ObjectVersion(S3ObjectName):
    pass


class SegmentConfidence(BaseModel):
    __root__: Annotated[float, Field(ge=50.0, le=100.0)]


class SegmentType(Enum):
    TECHNICAL_CUE = 'TECHNICAL_CUE'
    SHOT = 'SHOT'


class Timecode(String):
    pass


class ShotSegment(BaseModel):
    """
    Information about a shot detection segment detected in a video. For more information, see <a>SegmentDetection</a>.
    """

    Index: Optional[ULong] = None
    Confidence: Optional[SegmentConfidence] = None


class SegmentTypeInfo(BaseModel):
    """
    Information about the type of a segment requested in a call to <a>StartSegmentDetection</a>. An array of <code>SegmentTypeInfo</code> objects is returned by the response from <a>GetSegmentDetection</a>.
    """

    Type: Optional[SegmentType] = None
    ModelVersion: Optional[String] = None


class SegmentTypes(BaseModel):
    __root__: Annotated[List[SegmentType], Field(min_items=1)]


class StartTechnicalCueDetectionFilter(BaseModel):
    """
    Filters for the technical segments returned by <a>GetSegmentDetection</a>. For more information, see <a>StartSegmentDetectionFilters</a>.
    """

    MinSegmentConfidence: Optional[SegmentConfidence] = None
    BlackFrame: Optional[BlackFrame] = None


class StartShotDetectionFilter(BaseModel):
    """
    Filters for the shot detection segments returned by <code>GetSegmentDetection</code>. For more information, see <a>StartSegmentDetectionFilters</a>.
    """

    MinSegmentConfidence: Optional[SegmentConfidence] = None


class StartSegmentDetectionFilters(BaseModel):
    """
    Filters applied to the technical cue or shot detection segments. For more information, see <a>StartSegmentDetection</a>.
    """

    TechnicalCueFilter: Optional[StartTechnicalCueDetectionFilter] = None
    ShotFilter: Optional[StartShotDetectionFilter] = None


class StreamProcessor(BaseModel):
    """
    An object that recognizes faces in a streaming video. An Amazon Rekognition stream processor is created by a call to <a>CreateStreamProcessor</a>. The request parameters for <code>CreateStreamProcessor</code> describe the Kinesis video stream source for the streaming video, face recognition parameters, and where to stream the analysis resullts.
    """

    Name: Optional[StreamProcessorName] = None
    Status: Optional[StreamProcessorStatus] = None


class TagKey(BaseModel):
    __root__: Annotated[
        str,
        Field(
            max_length=128,
            min_length=1,
            regex='^(?!aws:)[\\p{L}\\p{Z}\\p{N}_.:/=+\\-@]*$',
        ),
    ]


class TagKeyList(BaseModel):
    __root__: Annotated[List[TagKey], Field(max_items=200, min_items=0)]


class TagValue(BaseModel):
    __root__: Annotated[
        str,
        Field(
            max_length=256, min_length=0, regex='^([\\p{L}\\p{Z}\\p{N}_.:/=+\\-@]*)$'
        ),
    ]


class TechnicalCueType(Enum):
    ColorBars = 'ColorBars'
    EndCredits = 'EndCredits'
    BlackFrames = 'BlackFrames'
    OpeningCredits = 'OpeningCredits'
    StudioLogo = 'StudioLogo'
    Slate = 'Slate'
    Content = 'Content'


class TextTypes(Enum):
    LINE = 'LINE'
    WORD = 'WORD'


class Url(String):
    pass


class VideoColorRange(Enum):
    FULL = 'FULL'
    LIMITED = 'LIMITED'


class CreateCollectionResponse(BaseModel):
    StatusCode: Optional[UInteger] = None
    CollectionArn: Optional[String] = None
    FaceModelVersion: Optional[String] = None


class CreateCollectionRequest(BaseModel):
    CollectionId: CollectionId
    Tags: Optional[TagMap] = None


class CreateProjectResponse(BaseModel):
    ProjectArn: Optional[ProjectArn] = None


class CreateProjectRequest(BaseModel):
    ProjectName: ProjectName


class CreateProjectVersionResponse(BaseModel):
    ProjectVersionArn: Optional[ProjectVersionArn] = None


class CreateStreamProcessorResponse(BaseModel):
    StreamProcessorArn: Optional[StreamProcessorArn] = None


class DeleteCollectionResponse(BaseModel):
    StatusCode: Optional[UInteger] = None


class DeleteCollectionRequest(BaseModel):
    CollectionId: CollectionId


class DeleteProjectResponse(BaseModel):
    Status: Optional[ProjectStatus] = None


class DeleteProjectRequest(BaseModel):
    ProjectArn: ProjectArn


class DeleteProjectVersionResponse(BaseModel):
    Status: Optional[ProjectVersionStatus] = None


class DeleteProjectVersionRequest(BaseModel):
    ProjectVersionArn: ProjectVersionArn


class DeleteStreamProcessorRequest(BaseModel):
    Name: StreamProcessorName


class DescribeCollectionResponse(BaseModel):
    FaceCount: Optional[ULong] = None
    FaceModelVersion: Optional[String] = None
    CollectionARN: Optional[String] = None
    CreationTimestamp: Optional[DateTime] = None


class DescribeCollectionRequest(BaseModel):
    CollectionId: CollectionId


class DescribeProjectVersionsRequest(BaseModel):
    ProjectArn: ProjectArn
    VersionNames: Optional[VersionNames] = None
    NextToken: Optional[ExtendedPaginationToken] = None
    MaxResults: Optional[ProjectVersionsPageSize] = None


class DescribeProjectsRequest(BaseModel):
    NextToken: Optional[ExtendedPaginationToken] = None
    MaxResults: Optional[ProjectsPageSize] = None


class DescribeStreamProcessorRequest(BaseModel):
    Name: StreamProcessorName


class GetCelebrityInfoRequest(BaseModel):
    Id: RekognitionUniqueId


class GetCelebrityRecognitionRequest(BaseModel):
    JobId: JobId
    MaxResults: Optional[MaxResults] = None
    NextToken: Optional[PaginationToken] = None
    SortBy: Optional[CelebrityRecognitionSortBy] = None


class GetContentModerationRequest(BaseModel):
    JobId: JobId
    MaxResults: Optional[MaxResults] = None
    NextToken: Optional[PaginationToken] = None
    SortBy: Optional[ContentModerationSortBy] = None


class GetFaceDetectionRequest(BaseModel):
    JobId: JobId
    MaxResults: Optional[MaxResults] = None
    NextToken: Optional[PaginationToken] = None


class GetFaceSearchRequest(BaseModel):
    JobId: JobId
    MaxResults: Optional[MaxResults] = None
    NextToken: Optional[PaginationToken] = None
    SortBy: Optional[FaceSearchSortBy] = None


class GetLabelDetectionRequest(BaseModel):
    JobId: JobId
    MaxResults: Optional[MaxResults] = None
    NextToken: Optional[PaginationToken] = None
    SortBy: Optional[ContentModerationSortBy] = None


class GetPersonTrackingRequest(BaseModel):
    JobId: JobId
    MaxResults: Optional[MaxResults] = None
    NextToken: Optional[PaginationToken] = None
    SortBy: Optional[FaceSearchSortBy] = None


class GetSegmentDetectionRequest(BaseModel):
    JobId: JobId
    MaxResults: Optional[MaxResults] = None
    NextToken: Optional[PaginationToken] = None


class GetTextDetectionRequest(BaseModel):
    JobId: JobId
    MaxResults: Optional[MaxResults] = None
    NextToken: Optional[PaginationToken] = None


class ListCollectionsResponse(BaseModel):
    CollectionIds: Optional[CollectionIdList] = None
    NextToken: Optional[PaginationToken] = None
    FaceModelVersions: Optional[FaceModelVersionList] = None


class ListCollectionsRequest(BaseModel):
    NextToken: Optional[PaginationToken] = None
    MaxResults: Optional[PageSize] = None


class ListFacesResponse(BaseModel):
    Faces: Optional[FaceList] = None
    NextToken: Optional[String] = None
    FaceModelVersion: Optional[String] = None


class ListFacesRequest(BaseModel):
    CollectionId: CollectionId
    NextToken: Optional[PaginationToken] = None
    MaxResults: Optional[PageSize] = None


class ListStreamProcessorsRequest(BaseModel):
    NextToken: Optional[PaginationToken] = None
    MaxResults: Optional[MaxResults] = None


class ListTagsForResourceResponse(BaseModel):
    Tags: Optional[TagMap] = None


class ListTagsForResourceRequest(BaseModel):
    ResourceArn: ResourceArn


class SearchFacesResponse(BaseModel):
    SearchedFaceId: Optional[FaceId] = None
    FaceMatches: Optional[FaceMatchList] = None
    FaceModelVersion: Optional[String] = None


class SearchFacesRequest(BaseModel):
    CollectionId: CollectionId
    FaceId: FaceId
    MaxFaces: Optional[MaxFaces] = None
    FaceMatchThreshold: Optional[Percent] = None


class SearchFacesByImageResponse(BaseModel):
    SearchedFaceBoundingBox: Optional[BoundingBox] = None
    SearchedFaceConfidence: Optional[Percent] = None
    FaceMatches: Optional[FaceMatchList] = None
    FaceModelVersion: Optional[String] = None


class StartCelebrityRecognitionResponse(BaseModel):
    JobId: Optional[JobId] = None


class StartContentModerationResponse(StartCelebrityRecognitionResponse):
    pass


class StartFaceDetectionResponse(StartCelebrityRecognitionResponse):
    pass


class StartFaceSearchResponse(StartCelebrityRecognitionResponse):
    pass


class StartLabelDetectionResponse(StartCelebrityRecognitionResponse):
    pass


class StartPersonTrackingResponse(StartCelebrityRecognitionResponse):
    pass


class StartProjectVersionResponse(DeleteProjectVersionResponse):
    pass


class StartProjectVersionRequest(BaseModel):
    ProjectVersionArn: ProjectVersionArn
    MinInferenceUnits: InferenceUnits


class StartSegmentDetectionResponse(StartCelebrityRecognitionResponse):
    pass


class StartStreamProcessorRequest(BaseModel):
    Name: StreamProcessorName


class StartTextDetectionResponse(StartCelebrityRecognitionResponse):
    pass


class StopProjectVersionResponse(DeleteProjectVersionResponse):
    pass


class StopProjectVersionRequest(BaseModel):
    ProjectVersionArn: ProjectVersionArn


class StopStreamProcessorRequest(BaseModel):
    Name: StreamProcessorName


class TagResourceRequest(BaseModel):
    ResourceArn: ResourceArn
    Tags: TagMap


class UntagResourceRequest(BaseModel):
    ResourceArn: ResourceArn
    TagKeys: TagKeyList


class ProtectiveEquipmentBodyPart(BaseModel):
    """
    Information about a body part detected by <a>DetectProtectiveEquipment</a> that contains PPE. An array of <code>ProtectiveEquipmentBodyPart</code> objects is returned for each person detected by <code>DetectProtectiveEquipment</code>.
    """

    Name: Optional[BodyPart] = None
    Confidence: Optional[Percent] = None
    EquipmentDetections: Optional[EquipmentDetections] = None


class BodyParts(BaseModel):
    __root__: List[ProtectiveEquipmentBodyPart]


class Urls(BaseModel):
    __root__: Annotated[List[Url], Field(max_items=255, min_items=0)]


class KnownGender(BaseModel):
    """
    The known gender identity for the celebrity that matches the provided ID.
    """

    Type: Optional[GenderType] = None


class Landmarks(BaseModel):
    __root__: List[Landmark]


class Pose(BaseModel):
    """
    Indicates the pose of the face as determined by its pitch, roll, and yaw.
    """

    Roll: Optional[Degree] = None
    Yaw: Optional[Degree] = None
    Pitch: Optional[Degree] = None


class Emotions(BaseModel):
    __root__: List[Emotion]


class OutputConfig(BaseModel):
    """
    The S3 bucket and folder location where training output is placed.
    """

    S3Bucket: Optional[S3Bucket] = None
    S3KeyPrefix: Optional[S3KeyPrefix] = None


class StreamProcessorInput(BaseModel):
    """
    Information about the source streaming video.
    """

    KinesisVideoStream: Optional[KinesisVideoStream] = None


class StreamProcessorOutput(BaseModel):
    """
    Information about the Amazon Kinesis Data Streams stream to which a Amazon Rekognition Video stream processor streams the results of a video analysis. For more information, see CreateStreamProcessor in the Amazon Rekognition Developer Guide.
    """

    KinesisDataStream: Optional[KinesisDataStream] = None


class StreamProcessorSettings(BaseModel):
    """
    Input parameters used to recognize faces in a streaming video analyzed by a Amazon Rekognition stream processor.
    """

    FaceSearch: Optional[FaceSearchSettings] = None


class FaceIdList(BaseModel):
    __root__: Annotated[List[FaceId], Field(max_items=4096, min_items=1)]


class ProjectDescriptions(BaseModel):
    __root__: List[ProjectDescription]


class HumanLoopConfig(BaseModel):
    """
    Sets up the flow definition the image will be sent to if one of the conditions is met. You can also set certain attributes of the image before review.
    """

    HumanLoopName: HumanLoopName
    FlowDefinitionArn: FlowDefinitionArn
    DataAttributes: Optional[HumanLoopDataAttributes] = None


class ProtectiveEquipmentSummarizationAttributes(BaseModel):
    """
    Specifies summary attributes to return from a call to <a>DetectProtectiveEquipment</a>. You can specify which types of PPE to summarize. You can also specify a minimum confidence value for detections. Summary information is returned in the <code>Summary</code> (<a>ProtectiveEquipmentSummary</a>) field of the response from <code>DetectProtectiveEquipment</code>. The summary includes which persons in an image were detected wearing the requested types of person protective equipment (PPE), which persons were detected as not wearing PPE, and the persons in which a determination could not be made. For more information, see <a>ProtectiveEquipmentSummary</a>.
    """

    MinConfidence: Percent
    RequiredEquipmentTypes: ProtectiveEquipmentTypes


class ProtectiveEquipmentSummary(BaseModel):
    """
    <p>Summary information for required items of personal protective equipment (PPE) detected on persons by a call to <a>DetectProtectiveEquipment</a>. You specify the required type of PPE in the <code>SummarizationAttributes</code> (<a>ProtectiveEquipmentSummarizationAttributes</a>) input parameter. The summary includes which persons were detected wearing the required personal protective equipment (<code>PersonsWithRequiredEquipment</code>), which persons were detected as not wearing the required PPE (<code>PersonsWithoutRequiredEquipment</code>), and the persons in which a determination could not be made (<code>PersonsIndeterminate</code>).</p> <p>To get a total for each category, use the size of the field array. For example, to find out how many people were detected as wearing the specified PPE, use the size of the <code>PersonsWithRequiredEquipment</code> array. If you want to find out more about a person, such as the location (<a>BoundingBox</a>) of the person on the image, use the person ID in each array element. Each person ID matches the ID field of a <a>ProtectiveEquipmentPerson</a> object returned in the <code>Persons</code> array by <code>DetectProtectiveEquipment</code>.</p>
    """

    PersonsWithRequiredEquipment: Optional[ProtectiveEquipmentPersonIds] = None
    PersonsWithoutRequiredEquipment: Optional[ProtectiveEquipmentPersonIds] = None
    PersonsIndeterminate: Optional[ProtectiveEquipmentPersonIds] = None


class RegionsOfInterest(BaseModel):
    __root__: Annotated[List[RegionOfInterest], Field(max_items=10, min_items=0)]


class DetectTextFilters(BaseModel):
    """
    A set of optional parameters that you can use to set the criteria that the text must meet to be included in your response. <code>WordFilter</code> looks at a word’s height, width, and minimum confidence. <code>RegionOfInterest</code> lets you set a specific region of the image to look for text in.
    """

    WordFilter: Optional[DetectionFilter] = None
    RegionsOfInterest: Optional[RegionsOfInterest] = None


class Gender(BaseModel):
    """
    <p>The predicted gender of a detected face. </p> <p>Amazon Rekognition makes gender binary (male/female) predictions based on the physical appearance of a face in a particular image. This kind of prediction is not designed to categorize a person’s gender identity, and you shouldn't use Amazon Rekognition to make such a determination. For example, a male actor wearing a long-haired wig and earrings for a role might be predicted as female.</p> <p>Using Amazon Rekognition to make gender binary predictions is best suited for use cases where aggregate gender distribution statistics need to be analyzed without identifying specific users. For example, the percentage of female users compared to male users on a social media platform. </p> <p>We don't recommend using gender binary predictions to make decisions that impact&#x2028; an individual's rights, privacy, or access to services.</p>
    """

    Value: Optional[GenderType] = None
    Confidence: Optional[Percent] = None


class Polygon(BaseModel):
    __root__: List[Point]


class VideoMetadata(BaseModel):
    """
    Information about a video that Amazon Rekognition analyzed. <code>Videometadata</code> is returned in every page of paginated responses from a Amazon Rekognition video operation.
    """

    Codec: Optional[String] = None
    DurationMillis: Optional[ULong] = None
    Format: Optional[String] = None
    FrameRate: Optional[Float] = None
    FrameHeight: Optional[ULong] = None
    FrameWidth: Optional[ULong] = None
    ColorRange: Optional[VideoColorRange] = None


class VideoMetadataList(BaseModel):
    __root__: List[VideoMetadata]


class SegmentTypesInfo(BaseModel):
    __root__: List[SegmentTypeInfo]


class S3Object(BaseModel):
    """
    <p>Provides the S3 bucket name and object name.</p> <p>The region for the S3 bucket containing the S3 object must match the region you use for Amazon Rekognition operations.</p> <p>For Amazon Rekognition to process an S3 object, the user must have permission to access the S3 object. For more information, see Resource-Based Policies in the Amazon Rekognition Developer Guide. </p>
    """

    Bucket: Optional[S3Bucket] = None
    Name: Optional[S3ObjectName] = None
    Version: Optional[S3ObjectVersion] = None


class HumanLoopActivationReasons(BaseModel):
    __root__: Annotated[List[HumanLoopActivationReason], Field(min_items=1)]


class Parents(BaseModel):
    __root__: List[Parent]


class Label(BaseModel):
    """
    <p>Structure containing details about the detected label, including the name, detected instances, parent labels, and level of confidence.</p> <p> </p>
    """

    Name: Optional[String] = None
    Confidence: Optional[Percent] = None
    Instances: Optional[Instances] = None
    Parents: Optional[Parents] = None


class LabelDetection(BaseModel):
    """
    Information about a label detected in a video analysis request and the time the label was detected in the video.
    """

    Timestamp: Optional[Timestamp] = None
    Label: Optional[Label] = None


class StreamProcessorList(BaseModel):
    __root__: List[StreamProcessor]


class ProtectiveEquipmentPerson(BaseModel):
    """
    A person detected by a call to <a>DetectProtectiveEquipment</a>. The API returns all persons detected in the input image in an array of <code>ProtectiveEquipmentPerson</code> objects.
    """

    BodyParts: Optional[BodyParts] = None
    BoundingBox: Optional[BoundingBox] = None
    Confidence: Optional[Percent] = None
    Id: Optional[UInteger] = None


class TechnicalCueSegment(BaseModel):
    """
    Information about a technical cue segment. For more information, see <a>SegmentDetection</a>.
    """

    Type: Optional[TechnicalCueType] = None
    Confidence: Optional[SegmentConfidence] = None


class SegmentDetection(BaseModel):
    """
    A technical cue or shot detection segment detected in a video. An array of <code>SegmentDetection</code> objects containing all segments detected in a stored video is returned by <a>GetSegmentDetection</a>.
    """

    Type: Optional[SegmentType] = None
    StartTimestampMillis: Optional[Timestamp] = None
    EndTimestampMillis: Optional[Timestamp] = None
    DurationMillis: Optional[ULong] = None
    StartTimecodeSMPTE: Optional[Timecode] = None
    EndTimecodeSMPTE: Optional[Timecode] = None
    DurationSMPTE: Optional[Timecode] = None
    TechnicalCueSegment: Optional[TechnicalCueSegment] = None
    ShotSegment: Optional[ShotSegment] = None
    StartFrameNumber: Optional[ULong] = None
    EndFrameNumber: Optional[ULong] = None
    DurationFrames: Optional[ULong] = None


class Video(BaseModel):
    """
    Video file stored in an Amazon S3 bucket. Amazon Rekognition video start operations such as <a>StartLabelDetection</a> use <code>Video</code> to specify a video for analysis. The supported file formats are .mp4, .mov and .avi.
    """

    S3Object: Optional[S3Object] = None


class StartTextDetectionFilters(DetectTextFilters):
    """
    Set of optional parameters that let you set the criteria text must meet to be included in your response. <code>WordFilter</code> looks at a word's height, width and minimum confidence. <code>RegionOfInterest</code> lets you set a specific region of the screen to look for text in.
    """

    pass


class CreateStreamProcessorRequest(BaseModel):
    Input: StreamProcessorInput
    Output: StreamProcessorOutput
    Name: StreamProcessorName
    Settings: StreamProcessorSettings
    RoleArn: RoleArn
    Tags: Optional[TagMap] = None


class DeleteFacesResponse(BaseModel):
    DeletedFaces: Optional[FaceIdList] = None


class DeleteFacesRequest(BaseModel):
    CollectionId: CollectionId
    FaceIds: FaceIdList


class DescribeProjectsResponse(BaseModel):
    ProjectDescriptions: Optional[ProjectDescriptions] = None
    NextToken: Optional[ExtendedPaginationToken] = None


class DescribeStreamProcessorResponse(BaseModel):
    Name: Optional[StreamProcessorName] = None
    StreamProcessorArn: Optional[StreamProcessorArn] = None
    Status: Optional[StreamProcessorStatus] = None
    StatusMessage: Optional[String] = None
    CreationTimestamp: Optional[DateTime] = None
    LastUpdateTimestamp: Optional[DateTime] = None
    Input: Optional[StreamProcessorInput] = None
    Output: Optional[StreamProcessorOutput] = None
    RoleArn: Optional[RoleArn] = None
    Settings: Optional[StreamProcessorSettings] = None


class GetCelebrityInfoResponse(BaseModel):
    Urls: Optional[Urls] = None
    Name: Optional[String] = None
    KnownGender: Optional[KnownGender] = None


class GetContentModerationResponse(BaseModel):
    JobStatus: Optional[VideoJobStatus] = None
    StatusMessage: Optional[StatusMessage] = None
    VideoMetadata: Optional[VideoMetadata] = None
    ModerationLabels: Optional[ContentModerationDetections] = None
    NextToken: Optional[PaginationToken] = None
    ModerationModelVersion: Optional[String] = None


class ListStreamProcessorsResponse(BaseModel):
    NextToken: Optional[PaginationToken] = None
    StreamProcessors: Optional[StreamProcessorList] = None


class StartCelebrityRecognitionRequest(BaseModel):
    Video: Video
    ClientRequestToken: Optional[ClientRequestToken] = None
    NotificationChannel: Optional[NotificationChannel] = None
    JobTag: Optional[JobTag] = None


class StartContentModerationRequest(BaseModel):
    Video: Video
    MinConfidence: Optional[Percent] = None
    ClientRequestToken: Optional[ClientRequestToken] = None
    NotificationChannel: Optional[NotificationChannel] = None
    JobTag: Optional[JobTag] = None


class StartFaceDetectionRequest(BaseModel):
    Video: Video
    ClientRequestToken: Optional[ClientRequestToken] = None
    NotificationChannel: Optional[NotificationChannel] = None
    FaceAttributes: Optional[Attribute] = None
    JobTag: Optional[JobTag] = None


class StartFaceSearchRequest(BaseModel):
    Video: Video
    ClientRequestToken: Optional[ClientRequestToken] = None
    FaceMatchThreshold: Optional[Percent] = None
    CollectionId: CollectionId
    NotificationChannel: Optional[NotificationChannel] = None
    JobTag: Optional[JobTag] = None


class StartLabelDetectionRequest(BaseModel):
    Video: Video
    ClientRequestToken: Optional[ClientRequestToken] = None
    MinConfidence: Optional[Percent] = None
    NotificationChannel: Optional[NotificationChannel] = None
    JobTag: Optional[JobTag] = None


class StartPersonTrackingRequest(BaseModel):
    Video: Video
    ClientRequestToken: Optional[ClientRequestToken] = None
    NotificationChannel: Optional[NotificationChannel] = None
    JobTag: Optional[JobTag] = None


class StartSegmentDetectionRequest(BaseModel):
    Video: Video
    ClientRequestToken: Optional[ClientRequestToken] = None
    NotificationChannel: Optional[NotificationChannel] = None
    JobTag: Optional[JobTag] = None
    Filters: Optional[StartSegmentDetectionFilters] = None
    SegmentTypes: SegmentTypes


class StartTextDetectionRequest(BaseModel):
    Video: Video
    ClientRequestToken: Optional[ClientRequestToken] = None
    NotificationChannel: Optional[NotificationChannel] = None
    JobTag: Optional[JobTag] = None
    Filters: Optional[StartTextDetectionFilters] = None


class GroundTruthManifest(Video):
    """
    The S3 bucket that contains an Amazon Sagemaker Ground Truth format manifest file.
    """

    pass


class Asset(BaseModel):
    """
    Assets are the images that you use to train and evaluate a model version. Assets can also contain validation information that you use to debug a failed model training.
    """

    GroundTruthManifest: Optional[GroundTruthManifest] = None


class Assets(BaseModel):
    __root__: List[Asset]


class ComparedFace(BaseModel):
    """
    Provides face metadata for target image faces that are analyzed by <code>CompareFaces</code> and <code>RecognizeCelebrities</code>.
    """

    BoundingBox: Optional[BoundingBox] = None
    Confidence: Optional[Percent] = None
    Landmarks: Optional[Landmarks] = None
    Pose: Optional[Pose] = None
    Quality: Optional[ImageQuality] = None
    Emotions: Optional[Emotions] = None
    Smile: Optional[Smile] = None


class Celebrity(BaseModel):
    """
    Provides information about a celebrity recognized by the <a>RecognizeCelebrities</a> operation.
    """

    Urls: Optional[Urls] = None
    Name: Optional[String] = None
    Id: Optional[RekognitionUniqueId] = None
    Face: Optional[ComparedFace] = None
    MatchConfidence: Optional[Percent] = None
    KnownGender: Optional[KnownGender] = None


class FaceDetail(BaseModel):
    """
    <p>Structure containing attributes of the face that the algorithm detected.</p> <p>A <code>FaceDetail</code> object contains either the default facial attributes or all facial attributes. The default attributes are <code>BoundingBox</code>, <code>Confidence</code>, <code>Landmarks</code>, <code>Pose</code>, and <code>Quality</code>.</p> <p> <a>GetFaceDetection</a> is the only Amazon Rekognition Video stored video operation that can return a <code>FaceDetail</code> object with all attributes. To specify which attributes to return, use the <code>FaceAttributes</code> input parameter for <a>StartFaceDetection</a>. The following Amazon Rekognition Video operations return only the default attributes. The corresponding Start operations don't have a <code>FaceAttributes</code> input parameter.</p> <ul> <li> <p>GetCelebrityRecognition</p> </li> <li> <p>GetPersonTracking</p> </li> <li> <p>GetFaceSearch</p> </li> </ul> <p>The Amazon Rekognition Image <a>DetectFaces</a> and <a>IndexFaces</a> operations can return all facial attributes. To specify which attributes to return, use the <code>Attributes</code> input parameter for <code>DetectFaces</code>. For <code>IndexFaces</code>, use the <code>DetectAttributes</code> input parameter.</p>
    """

    BoundingBox: Optional[BoundingBox] = None
    AgeRange: Optional[AgeRange] = None
    Smile: Optional[Smile] = None
    Eyeglasses: Optional[Eyeglasses] = None
    Sunglasses: Optional[Sunglasses] = None
    Gender: Optional[Gender] = None
    Beard: Optional[Beard] = None
    Mustache: Optional[Mustache] = None
    EyesOpen: Optional[EyeOpen] = None
    MouthOpen: Optional[MouthOpen] = None
    Emotions: Optional[Emotions] = None
    Landmarks: Optional[Landmarks] = None
    Pose: Optional[Pose] = None
    Quality: Optional[ImageQuality] = None
    Confidence: Optional[Percent] = None


class CelebrityDetail(BaseModel):
    """
    Information about a recognized celebrity.
    """

    Urls: Optional[Urls] = None
    Name: Optional[String] = None
    Id: Optional[RekognitionUniqueId] = None
    Confidence: Optional[Percent] = None
    BoundingBox: Optional[BoundingBox] = None
    Face: Optional[FaceDetail] = None


class CelebrityList(BaseModel):
    __root__: List[Celebrity]


class CelebrityRecognition(BaseModel):
    """
    Information about a detected celebrity and the time the celebrity was detected in a stored video. For more information, see GetCelebrityRecognition in the Amazon Rekognition Developer Guide.
    """

    Timestamp: Optional[Timestamp] = None
    Celebrity: Optional[CelebrityDetail] = None


class CelebrityRecognitions(BaseModel):
    __root__: List[CelebrityRecognition]


class CompareFacesMatch(BaseModel):
    """
    Provides information about a face in a target image that matches the source image face analyzed by <code>CompareFaces</code>. The <code>Face</code> property contains the bounding box of the face in the target image. The <code>Similarity</code> property is the confidence that the source image face matches the face in the bounding box.
    """

    Similarity: Optional[Percent] = None
    Face: Optional[ComparedFace] = None


class CompareFacesMatchList(BaseModel):
    __root__: List[CompareFacesMatch]


class Image(BaseModel):
    """
    <p>Provides the input image either as bytes or an S3 object.</p> <p>You pass image bytes to an Amazon Rekognition API operation by using the <code>Bytes</code> property. For example, you would use the <code>Bytes</code> property to pass an image loaded from a local file system. Image bytes passed by using the <code>Bytes</code> property must be base64-encoded. Your code may not need to encode image bytes if you are using an AWS SDK to call Amazon Rekognition API operations. </p> <p>For more information, see Analyzing an Image Loaded from a Local File System in the Amazon Rekognition Developer Guide.</p> <p> You pass images stored in an S3 bucket to an Amazon Rekognition API operation by using the <code>S3Object</code> property. Images stored in an S3 bucket do not need to be base64-encoded.</p> <p>The region for the S3 bucket containing the S3 object must match the region you use for Amazon Rekognition operations.</p> <p>If you use the AWS CLI to call Amazon Rekognition operations, passing image bytes using the Bytes property is not supported. You must first upload the image to an Amazon S3 bucket and then call the operation using the S3Object property.</p> <p>For Amazon Rekognition to process an S3 object, the user must have permission to access the S3 object. For more information, see Resource Based Policies in the Amazon Rekognition Developer Guide. </p>
    """

    Bytes: Optional[ImageBlob] = None
    S3Object: Optional[S3Object] = None


class CompareFacesUnmatchList(BaseModel):
    __root__: List[ComparedFace]


class ComparedFaceList(CompareFacesUnmatchList):
    pass


class TrainingData(BaseModel):
    """
    The dataset used for training.
    """

    Assets: Optional[Assets] = None


class TestingData(BaseModel):
    """
    The dataset used for testing. Optionally, if <code>AutoCreate</code> is set, Amazon Rekognition Custom Labels creates a testing dataset using an 80/20 split of the training dataset.
    """

    Assets: Optional[Assets] = None
    AutoCreate: Optional[Boolean] = None


class Geometry(BaseModel):
    """
    Information about where an object (<a>DetectCustomLabels</a>) or text (<a>DetectText</a>) is located on an image.
    """

    BoundingBox: Optional[BoundingBox] = None
    Polygon: Optional[Polygon] = None


class CustomLabel(BaseModel):
    """
    A custom label detected in an image by a call to <a>DetectCustomLabels</a>.
    """

    Name: Optional[String] = None
    Confidence: Optional[Percent] = None
    Geometry: Optional[Geometry] = None


class CustomLabels(BaseModel):
    __root__: List[CustomLabel]


class FaceDetailList(BaseModel):
    __root__: List[FaceDetail]


class Labels(BaseModel):
    __root__: List[Label]


class HumanLoopActivationOutput(BaseModel):
    """
    Shows the results of the human in the loop evaluation. If there is no HumanLoopArn, the input did not trigger human review.
    """

    HumanLoopArn: Optional[HumanLoopArn] = None
    HumanLoopActivationReasons: Optional[HumanLoopActivationReasons] = None
    HumanLoopActivationConditionsEvaluationResults: Optional[
        HumanLoopActivationConditionsEvaluationResults
    ] = None


class ProtectiveEquipmentPersons(BaseModel):
    __root__: List[ProtectiveEquipmentPerson]


class Summary1(Video):
    """
    <p>The S3 bucket that contains the training summary. The training summary includes aggregated evaluation metrics for the entire testing dataset and metrics for each individual label. </p> <p>You get the training summary S3 bucket location by calling <a>DescribeProjectVersions</a>. </p>
    """

    pass


class EvaluationResult(BaseModel):
    """
    The evaluation results for the training of a model.
    """

    F1Score: Optional[Float] = None
    Summary: Optional[Summary1] = None


class FaceDetection(BaseModel):
    """
    Information about a face detected in a video analysis request and the time the face was detected in the video.
    """

    Timestamp: Optional[Timestamp] = None
    Face: Optional[FaceDetail] = None


class FaceDetections(BaseModel):
    __root__: List[FaceDetection]


class FaceRecord(BaseModel):
    """
    Object containing both the face metadata (stored in the backend database), and facial attributes that are detected but aren't stored in the database.
    """

    Face: Optional[Face3] = None
    FaceDetail: Optional[FaceDetail] = None


class FaceRecordList(BaseModel):
    __root__: List[FaceRecord]


class LabelDetections(BaseModel):
    __root__: List[LabelDetection]


class SegmentDetections(BaseModel):
    __root__: List[SegmentDetection]


class PersonDetail(BaseModel):
    """
    Details about a person detected in a video analysis request.
    """

    Index: Optional[PersonIndex] = None
    BoundingBox: Optional[BoundingBox] = None
    Face: Optional[FaceDetail] = None


class PersonDetection(BaseModel):
    """
    <p>Details and path tracking information for a single time a person's path is tracked in a video. Amazon Rekognition operations that track people's paths return an array of <code>PersonDetection</code> objects with elements for each time a person's path is tracked in a video. </p> <p>For more information, see GetPersonTracking in the Amazon Rekognition Developer Guide. </p>
    """

    Timestamp: Optional[Timestamp] = None
    Person: Optional[PersonDetail] = None


class PersonMatch(BaseModel):
    """
    Information about a person whose face matches a face(s) in an Amazon Rekognition collection. Includes information about the faces in the Amazon Rekognition collection (<a>FaceMatch</a>), information about the person (<a>PersonDetail</a>), and the time stamp for when the person was detected in a video. An array of <code>PersonMatch</code> objects is returned by <a>GetFaceSearch</a>.
    """

    Timestamp: Optional[Timestamp] = None
    Person: Optional[PersonDetail] = None
    FaceMatches: Optional[FaceMatchList] = None


class ValidationData(TrainingData):
    """
    <p>Contains the Amazon S3 bucket location of the validation data for a model training job. </p> <p>The validation data includes error information for individual JSON lines in the dataset. For more information, see Debugging a Failed Model Training in the Amazon Rekognition Custom Labels Developer Guide. </p> <p>You get the <code>ValidationData</code> object for the training dataset (<a>TrainingDataResult</a>) and the test dataset (<a>TestingDataResult</a>) by calling <a>DescribeProjectVersions</a>. </p> <p>The assets array contains a single <a>Asset</a> object. The <a>GroundTruthManifest</a> field of the Asset object contains the S3 bucket location of the validation data. </p>
    """

    pass


class TextDetection(BaseModel):
    """
    <p>Information about a word or line of text detected by <a>DetectText</a>.</p> <p>The <code>DetectedText</code> field contains the text that Amazon Rekognition detected in the image. </p> <p>Every word and line has an identifier (<code>Id</code>). Each word belongs to a line and has a parent identifier (<code>ParentId</code>) that identifies the line of text in which the word appears. The word <code>Id</code> is also an index for the word within a line of words. </p> <p>For more information, see Detecting Text in the Amazon Rekognition Developer Guide.</p>
    """

    DetectedText: Optional[String] = None
    Type: Optional[TextTypes] = None
    Id: Optional[UInteger] = None
    ParentId: Optional[UInteger] = None
    Confidence: Optional[Percent] = None
    Geometry: Optional[Geometry] = None


class TextDetectionResult(BaseModel):
    """
    Information about text detected in a video. Incudes the detected text, the time in milliseconds from the start of the video that the text was detected, and where it was detected on the screen.
    """

    Timestamp: Optional[Timestamp] = None
    TextDetection: Optional[TextDetection] = None


class UnindexedFace(BaseModel):
    """
    A face that <a>IndexFaces</a> detected, but didn't index. Use the <code>Reasons</code> response attribute to determine why a face wasn't indexed.
    """

    Reasons: Optional[Reasons] = None
    FaceDetail: Optional[FaceDetail] = None


class CompareFacesResponse(BaseModel):
    SourceImageFace: Optional[ComparedSourceImageFace] = None
    FaceMatches: Optional[CompareFacesMatchList] = None
    UnmatchedFaces: Optional[CompareFacesUnmatchList] = None
    SourceImageOrientationCorrection: Optional[OrientationCorrection] = None
    TargetImageOrientationCorrection: Optional[OrientationCorrection] = None


class CompareFacesRequest(BaseModel):
    SourceImage: Image
    TargetImage: Image
    SimilarityThreshold: Optional[Percent] = None
    QualityFilter: Optional[QualityFilter] = None


class CreateProjectVersionRequest(BaseModel):
    ProjectArn: ProjectArn
    VersionName: VersionName
    OutputConfig: OutputConfig
    TrainingData: TrainingData
    TestingData: TestingData
    Tags: Optional[TagMap] = None
    KmsKeyId: Optional[KmsKeyId] = None


class DetectCustomLabelsResponse(BaseModel):
    CustomLabels: Optional[CustomLabels] = None


class DetectCustomLabelsRequest(BaseModel):
    ProjectVersionArn: ProjectVersionArn
    Image: Image
    MaxResults: Optional[UInteger] = None
    MinConfidence: Optional[Percent] = None


class DetectFacesResponse(BaseModel):
    FaceDetails: Optional[FaceDetailList] = None
    OrientationCorrection: Optional[OrientationCorrection] = None


class DetectFacesRequest(BaseModel):
    Image: Image
    Attributes: Optional[Attributes] = None


class DetectLabelsResponse(BaseModel):
    Labels: Optional[Labels] = None
    OrientationCorrection: Optional[OrientationCorrection] = None
    LabelModelVersion: Optional[String] = None


class DetectLabelsRequest(BaseModel):
    Image: Image
    MaxLabels: Optional[UInteger] = None
    MinConfidence: Optional[Percent] = None


class DetectModerationLabelsResponse(BaseModel):
    ModerationLabels: Optional[ModerationLabels] = None
    ModerationModelVersion: Optional[String] = None
    HumanLoopActivationOutput: Optional[HumanLoopActivationOutput] = None


class DetectModerationLabelsRequest(BaseModel):
    Image: Image
    MinConfidence: Optional[Percent] = None
    HumanLoopConfig: Optional[HumanLoopConfig] = None


class DetectProtectiveEquipmentResponse(BaseModel):
    ProtectiveEquipmentModelVersion: Optional[String] = None
    Persons: Optional[ProtectiveEquipmentPersons] = None
    Summary: Optional[ProtectiveEquipmentSummary] = None


class DetectProtectiveEquipmentRequest(BaseModel):
    Image: Image
    SummarizationAttributes: Optional[ProtectiveEquipmentSummarizationAttributes] = None


class DetectTextRequest(BaseModel):
    Image: Image
    Filters: Optional[DetectTextFilters] = None


class GetCelebrityRecognitionResponse(BaseModel):
    JobStatus: Optional[VideoJobStatus] = None
    StatusMessage: Optional[StatusMessage] = None
    VideoMetadata: Optional[VideoMetadata] = None
    NextToken: Optional[PaginationToken] = None
    Celebrities: Optional[CelebrityRecognitions] = None


class GetFaceDetectionResponse(BaseModel):
    JobStatus: Optional[VideoJobStatus] = None
    StatusMessage: Optional[StatusMessage] = None
    VideoMetadata: Optional[VideoMetadata] = None
    NextToken: Optional[PaginationToken] = None
    Faces: Optional[FaceDetections] = None


class GetLabelDetectionResponse(BaseModel):
    JobStatus: Optional[VideoJobStatus] = None
    StatusMessage: Optional[StatusMessage] = None
    VideoMetadata: Optional[VideoMetadata] = None
    NextToken: Optional[PaginationToken] = None
    Labels: Optional[LabelDetections] = None
    LabelModelVersion: Optional[String] = None


class GetSegmentDetectionResponse(BaseModel):
    JobStatus: Optional[VideoJobStatus] = None
    StatusMessage: Optional[StatusMessage] = None
    VideoMetadata: Optional[VideoMetadataList] = None
    AudioMetadata: Optional[AudioMetadataList] = None
    NextToken: Optional[PaginationToken] = None
    Segments: Optional[SegmentDetections] = None
    SelectedSegmentTypes: Optional[SegmentTypesInfo] = None


class IndexFacesRequest(BaseModel):
    CollectionId: CollectionId
    Image: Image
    ExternalImageId: Optional[ExternalImageId] = None
    DetectionAttributes: Optional[Attributes] = None
    MaxFaces: Optional[MaxFacesToIndex] = None
    QualityFilter: Optional[QualityFilter] = None


class RecognizeCelebritiesResponse(BaseModel):
    CelebrityFaces: Optional[CelebrityList] = None
    UnrecognizedFaces: Optional[ComparedFaceList] = None
    OrientationCorrection: Optional[OrientationCorrection] = None


class RecognizeCelebritiesRequest(BaseModel):
    Image: Image


class SearchFacesByImageRequest(BaseModel):
    CollectionId: CollectionId
    Image: Image
    MaxFaces: Optional[MaxFaces] = None
    FaceMatchThreshold: Optional[Percent] = None
    QualityFilter: Optional[QualityFilter] = None


class TextDetectionList(BaseModel):
    __root__: List[TextDetection]


class PersonMatches(BaseModel):
    __root__: List[PersonMatch]


class PersonDetections(BaseModel):
    __root__: List[PersonDetection]


class TextDetectionResults(BaseModel):
    __root__: List[TextDetectionResult]


class UnindexedFaces(BaseModel):
    __root__: List[UnindexedFace]


class TrainingDataResult(BaseModel):
    """
    Sagemaker Groundtruth format manifest files for the input, output and validation datasets that are used and created during testing.
    """

    Input: Optional[TrainingData] = None
    Output: Optional[TrainingData] = None
    Validation: Optional[ValidationData] = None


class TestingDataResult(BaseModel):
    """
    Sagemaker Groundtruth format manifest files for the input, output and validation datasets that are used and created during testing.
    """

    Input: Optional[TestingData] = None
    Output: Optional[TestingData] = None
    Validation: Optional[ValidationData] = None


class ProjectVersionDescription(BaseModel):
    """
    The description of a version of a model.
    """

    ProjectVersionArn: Optional[ProjectVersionArn] = None
    CreationTimestamp: Optional[DateTime] = None
    MinInferenceUnits: Optional[InferenceUnits] = None
    Status: Optional[ProjectVersionStatus] = None
    StatusMessage: Optional[StatusMessage] = None
    BillableTrainingTimeInSeconds: Optional[ULong] = None
    TrainingEndTimestamp: Optional[DateTime] = None
    OutputConfig: Optional[OutputConfig] = None
    TrainingDataResult: Optional[TrainingDataResult] = None
    TestingDataResult: Optional[TestingDataResult] = None
    EvaluationResult: Optional[EvaluationResult] = None
    ManifestSummary: Optional[GroundTruthManifest] = None
    KmsKeyId: Optional[KmsKeyId] = None


class DetectTextResponse(BaseModel):
    TextDetections: Optional[TextDetectionList] = None
    TextModelVersion: Optional[String] = None


class GetFaceSearchResponse(BaseModel):
    JobStatus: Optional[VideoJobStatus] = None
    StatusMessage: Optional[StatusMessage] = None
    NextToken: Optional[PaginationToken] = None
    VideoMetadata: Optional[VideoMetadata] = None
    Persons: Optional[PersonMatches] = None


class GetPersonTrackingResponse(BaseModel):
    JobStatus: Optional[VideoJobStatus] = None
    StatusMessage: Optional[StatusMessage] = None
    VideoMetadata: Optional[VideoMetadata] = None
    NextToken: Optional[PaginationToken] = None
    Persons: Optional[PersonDetections] = None


class GetTextDetectionResponse(BaseModel):
    JobStatus: Optional[VideoJobStatus] = None
    StatusMessage: Optional[StatusMessage] = None
    VideoMetadata: Optional[VideoMetadata] = None
    TextDetections: Optional[TextDetectionResults] = None
    NextToken: Optional[PaginationToken] = None
    TextModelVersion: Optional[String] = None


class IndexFacesResponse(BaseModel):
    FaceRecords: Optional[FaceRecordList] = None
    OrientationCorrection: Optional[OrientationCorrection] = None
    FaceModelVersion: Optional[String] = None
    UnindexedFaces: Optional[UnindexedFaces] = None


class ProjectVersionDescriptions(BaseModel):
    __root__: List[ProjectVersionDescription]


class DescribeProjectVersionsResponse(BaseModel):
    ProjectVersionDescriptions: Optional[ProjectVersionDescriptions] = None
    NextToken: Optional[ExtendedPaginationToken] = None
